{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poc/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Keras modules\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_orig shape: (28, 28, 1, 9000)\n",
      "X_test_orig shape: (28, 28, 1, 1000)\n",
      "Y_train_orig shape: (9000,)\n",
      "Y_test_orig shape: (1000,) \n",
      "\n",
      "X_train shape: (9000, 28, 28, 1)\n",
      "X_test shape: (1000, 28, 28, 1)\n",
      "Y_train shape: (9000, 10)\n",
      "Y_test shape: (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the datasets\n",
    "sample_full_data = loadmat('Digit_Dataset_Full.mat')\n",
    "label_train_data = pd.read_csv(\"Digit_Dataset_Full_Train_Labels.csv\")\n",
    "label_test_data = pd.read_csv(\"Digit_Dataset_Full_Test_Labels.csv\")\n",
    "\n",
    "# Get data from the datasets\n",
    "X_train_orig = sample_full_data['Image'][0, 0][0]\n",
    "X_test_orig = sample_full_data['Image'][0, 0][1]\n",
    "Y_train_orig = label_train_data.values[:, 0]\n",
    "Y_test_orig = label_test_data.values[:, 0]\n",
    "\n",
    "# Print details of the orignal data\n",
    "print(\"X_train_orig shape: \" + str(X_train_orig.shape))\n",
    "print(\"X_test_orig shape: \" + str(X_test_orig.shape))\n",
    "print(\"Y_train_orig shape: \" + str(Y_train_orig.shape))\n",
    "print(\"Y_test_orig shape: \" + str(Y_test_orig.shape), \"\\n\")\n",
    "\n",
    "# Reshape the input data for keras\n",
    "split_fraction = 0.9     # should be greater than 0.5\n",
    "train_set_len = math.ceil((X_train_orig.shape[3] + X_test_orig.shape[3]) * split_fraction)\n",
    "test_set_len = X_train_orig.shape[3] + X_test_orig.shape[3] - train_set_len\n",
    "X_train = np.zeros((train_set_len, X_train_orig.shape[0], X_train_orig.shape[1], X_train_orig.shape[2]))\n",
    "X_test = np.zeros((test_set_len, X_train_orig.shape[0], X_train_orig.shape[1], X_train_orig.shape[2]))\n",
    "Y_train = np.zeros((train_set_len, 1))\n",
    "Y_test = np.zeros((test_set_len, 1))\n",
    "\n",
    "# Split into train and test, by the given split fraction\n",
    "for i in range(train_set_len + test_set_len):\n",
    "    if i < train_set_len:\n",
    "        if i < X_train_orig.shape[3]:\n",
    "            X_train[i] = X_train_orig[:, :, :, i]\n",
    "            Y_train[i] = Y_train_orig[i]\n",
    "        else:\n",
    "            X_train[i] = X_test_orig[:, :, :, i - X_train_orig.shape[3]]\n",
    "            Y_train[i] = Y_test_orig[i - X_train_orig.shape[3]]\n",
    "        \n",
    "    else:\n",
    "        if i < X_train_orig.shape[3]:\n",
    "            X_test[i - train_set_len] = X_train_orig[:, :, :, i]\n",
    "            Y_test[i - train_set_len] = Y_train_orig[i]\n",
    "        else:\n",
    "            X_test[i - train_set_len] = X_test_orig[:, :, :, i - X_train_orig.shape[3]]\n",
    "            Y_test[i - train_set_len] = Y_test_orig[i - X_train_orig.shape[3]]\n",
    "\n",
    "# Convert the integer labels into one-hot\n",
    "Y_train = to_categorical(Y_train, num_classes=10, dtype='float32')\n",
    "Y_test = to_categorical(Y_test, num_classes=10, dtype='float32')\n",
    "\n",
    "# Print details of the reshaped data\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape))\n",
    "\n",
    "# Create an image generator class\n",
    "imgGentrain = ImageDataGenerator()\n",
    "imgGentest = ImageDataGenerator()\n",
    "\n",
    "train_batch = imgGentrain.flow(\n",
    "    x=X_train,\n",
    "    y=Y_train,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=1)\n",
    "test_batch = imgGentest.flow(\n",
    "    x=X_test,\n",
    "    y=Y_test,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 3600    # just some image for preview\n",
    "plt.imshow(X_train[index, :, :, 0])\n",
    "print(\"y = \" + str(np.squeeze(Y_train[index, :])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Create a Model (With Batch Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lr=0.001, beta_1=0.9):\n",
    "    try:\n",
    "        del model\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Create a model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add a convolutional layer\n",
    "    model.add(Conv2D(filters=32, kernel_size=5, strides=(1, 1), padding='valid', input_shape=(28, 28, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "\n",
    "    # 2. Add a convolution layer\n",
    "    model.add(Conv2D(filters=16, kernel_size=3, strides=(1, 1), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "\n",
    "    # Flatten the output\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add a dense layer\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    optAdam = optimizers.Adam(lr=lr, beta_1=beta_1, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(optimizer=optAdam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Evaluate the Current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_with(lr=0.001, beta_1=0.9):\n",
    "    # Create the model using a specified hyperparameters.\n",
    "    model = create_model(lr=lr, beta_1=beta_1)\n",
    "    \n",
    "    # Train the model with the train dataset.\n",
    "    model.fit_generator(\n",
    "        generator=train_batch,\n",
    "        steps_per_epoch=len(train_batch),\n",
    "        epochs=3)\n",
    "    \n",
    "    # print the test accuracy\n",
    "    score = model.evaluate(X_test, Y_test, verbose = 0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    # return the test accuracy\n",
    "    return score[1]\n",
    "\n",
    "fit_with_partial = partial(fit_with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  beta_1   |    lr     |\n",
      "-------------------------------------------------\n",
      "WARNING:tensorflow:From /home/poc/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/poc/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 21s 74ms/step - loss: 3.6921 - acc: 0.6659\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 18s 62ms/step - loss: 3.2720 - acc: 0.7848\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 2.3819 - acc: 0.8241\n",
      "Test loss: 0.09564479201845824\n",
      "Test accuracy: 0.971\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.971   \u001b[0m | \u001b[0m 0.8834  \u001b[0m | \u001b[0m 0.007231\u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 18s 64ms/step - loss: 0.5866 - acc: 0.8183\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 22s 78ms/step - loss: 0.0882 - acc: 0.9709\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 24s 86ms/step - loss: 0.0131 - acc: 0.9977\n",
      "Test loss: 0.006796402305364609\n",
      "Test accuracy: 0.998\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.998   \u001b[0m | \u001b[95m 0.8     \u001b[0m | \u001b[95m 0.003093\u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 20s 69ms/step - loss: 0.6574 - acc: 0.7941\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 25s 89ms/step - loss: 0.0981 - acc: 0.9802\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 20s 70ms/step - loss: 0.0341 - acc: 0.9960\n",
      "Test loss: 0.030636158142238856\n",
      "Test accuracy: 0.995\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.995   \u001b[0m | \u001b[0m 0.8294  \u001b[0m | \u001b[0m 0.001014\u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 19s 69ms/step - loss: 0.6193 - acc: 0.8108\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 21s 73ms/step - loss: 0.0577 - acc: 0.9852\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 19s 68ms/step - loss: 0.0401 - acc: 0.9888\n",
      "Test loss: 0.009827798534883187\n",
      "Test accuracy: 0.997\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.997   \u001b[0m | \u001b[0m 0.8373  \u001b[0m | \u001b[0m 0.003521\u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 18s 65ms/step - loss: 0.6721 - acc: 0.8057\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 16s 55ms/step - loss: 0.0679 - acc: 0.9792\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 15s 52ms/step - loss: 0.0723 - acc: 0.9778\n",
      "Test loss: 0.12063172279548598\n",
      "Test accuracy: 0.963\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.963   \u001b[0m | \u001b[0m 0.8794  \u001b[0m | \u001b[0m 0.005434\u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 16s 57ms/step - loss: 2.2387 - acc: 0.7450\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 15s 52ms/step - loss: 0.0764 - acc: 0.9764\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 15s 52ms/step - loss: 0.0360 - acc: 0.9906\n",
      "Test loss: 0.03859443750977516\n",
      "Test accuracy: 0.994\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.994   \u001b[0m | \u001b[0m 0.8838  \u001b[0m | \u001b[0m 0.006884\u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 16s 56ms/step - loss: 1.7068 - acc: 0.7617\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 15s 52ms/step - loss: 0.0918 - acc: 0.9726\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 15s 53ms/step - loss: 0.0206 - acc: 0.9945\n",
      "Test loss: 0.038596218196558764\n",
      "Test accuracy: 0.991\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.991   \u001b[0m | \u001b[0m 0.8409  \u001b[0m | \u001b[0m 0.008793\u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 22s 76ms/step - loss: 0.7717 - acc: 0.8208\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 19s 67ms/step - loss: 0.0764 - acc: 0.9751\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 19s 67ms/step - loss: 0.0402 - acc: 0.9876\n",
      "Test loss: 0.0062416974066291\n",
      "Test accuracy: 0.998\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.998   \u001b[0m | \u001b[0m 0.8055  \u001b[0m | \u001b[0m 0.006738\u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 20s 71ms/step - loss: 0.6634 - acc: 0.8142\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 19s 67ms/step - loss: 0.0951 - acc: 0.9709\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 19s 67ms/step - loss: 0.0266 - acc: 0.9919\n",
      "Test loss: 0.02194678747165017\n",
      "Test accuracy: 0.994\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.994   \u001b[0m | \u001b[0m 0.8835  \u001b[0m | \u001b[0m 0.005631\u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 20s 72ms/step - loss: 0.4977 - acc: 0.8445\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 19s 69ms/step - loss: 0.0648 - acc: 0.9827\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 20s 69ms/step - loss: 0.0497 - acc: 0.9857\n",
      "Test loss: 0.11759197735682392\n",
      "Test accuracy: 0.969\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.969   \u001b[0m | \u001b[0m 0.8281  \u001b[0m | \u001b[0m 0.002061\u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 22s 76ms/step - loss: 2.4398 - acc: 0.6154\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 15s 53ms/step - loss: 0.1337 - acc: 0.9574\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 15s 53ms/step - loss: 0.0635 - acc: 0.9792\n",
      "Test loss: 0.03985049867187627\n",
      "Test accuracy: 0.987\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.987   \u001b[0m | \u001b[0m 0.9238  \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 16s 58ms/step - loss: nan - acc: 0.1018\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 15s 53ms/step - loss: nan - acc: 0.0987\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 15s 53ms/step - loss: nan - acc: 0.0985\n",
      "Test loss: nan\n",
      "Test accuracy: 0.1\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 1.6894 - acc: 0.4381\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 15s 54ms/step - loss: 0.8803 - acc: 0.7475\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 15s 54ms/step - loss: 0.5482 - acc: 0.8516\n",
      "Test loss: 0.4763501634597778\n",
      "Test accuracy: 0.886\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.886   \u001b[0m | \u001b[0m 0.9575  \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 17s 62ms/step - loss: 1.6713 - acc: 0.4390\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 15s 54ms/step - loss: 0.8608 - acc: 0.7570\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 15s 55ms/step - loss: 0.5507 - acc: 0.8598\n",
      "Test loss: 0.45985244297981265\n",
      "Test accuracy: 0.882\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.882   \u001b[0m | \u001b[0m 0.9061  \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 1.6459 - acc: 0.4539\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 20s 71ms/step - loss: 0.8536 - acc: 0.7550\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 21s 75ms/step - loss: 0.5326 - acc: 0.8710\n",
      "Test loss: 0.46304218673706055\n",
      "Test accuracy: 0.889\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.889   \u001b[0m | \u001b[0m 0.8584  \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 22s 76ms/step - loss: 1.8277 - acc: 0.6468\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 20s 71ms/step - loss: 0.1705 - acc: 0.9496\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 19s 68ms/step - loss: 0.0803 - acc: 0.9768\n",
      "Test loss: 0.07756807271763683\n",
      "Test accuracy: 0.964\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.964   \u001b[0m | \u001b[0m 0.9746  \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 21s 73ms/step - loss: 4.3146 - acc: 0.4450\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 20s 72ms/step - loss: 0.3303 - acc: 0.8924\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 23s 83ms/step - loss: 0.1084 - acc: 0.9678\n",
      "Test loss: 0.14060125095583498\n",
      "Test accuracy: 0.956\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.956   \u001b[0m | \u001b[0m 0.9445  \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 24s 85ms/step - loss: 2.3155 - acc: 0.7216\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 20s 70ms/step - loss: 1.7141 - acc: 0.8757\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 20s 71ms/step - loss: 1.6485 - acc: 0.8889\n",
      "Test loss: 1.622260201461293\n",
      "Test accuracy: 0.899\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.899   \u001b[0m | \u001b[0m 0.9013  \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "282/282 [==============================] - 20s 72ms/step - loss: 1.7040 - acc: 0.4383\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 19s 66ms/step - loss: 0.8624 - acc: 0.7502\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 18s 64ms/step - loss: 0.5462 - acc: 0.8610\n",
      "Test loss: 0.4608350868225098\n",
      "Test accuracy: 0.878\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.878   \u001b[0m | \u001b[0m 0.9335  \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n",
      "Epoch 1/3\n",
      "282/282 [==============================] - 20s 72ms/step - loss: 1.5992 - acc: 0.4848\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 18s 65ms/step - loss: 0.8468 - acc: 0.7564\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.5337 - acc: 0.8681\n",
      "Test loss: 0.4637343616485596\n",
      "Test accuracy: 0.88\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.88    \u001b[0m | \u001b[0m 0.9763  \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n",
      "=================================================\n",
      "Iteration 0: \n",
      "\t{'target': 0.971, 'params': {'beta_1': 0.8834044009405149, 'lr': 0.007231212485077366}}\n",
      "Iteration 1: \n",
      "\t{'target': 0.998, 'params': {'beta_1': 0.800022874963469, 'lr': 0.003093092469055214}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.995, 'params': {'beta_1': 0.8293511781634226, 'lr': 0.0010141520882110983}}\n",
      "Iteration 3: \n",
      "\t{'target': 0.997, 'params': {'beta_1': 0.8372520422755342, 'lr': 0.003521051197726173}}\n",
      "Iteration 4: \n",
      "\t{'target': 0.963, 'params': {'beta_1': 0.879353494846134, 'lr': 0.005434285666633234}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.994, 'params': {'beta_1': 0.883838902880659, 'lr': 0.00688367305392792}}\n",
      "Iteration 6: \n",
      "\t{'target': 0.991, 'params': {'beta_1': 0.8408904499463035, 'lr': 0.00879336262027036}}\n",
      "Iteration 7: \n",
      "\t{'target': 0.998, 'params': {'beta_1': 0.8054775186395853, 'lr': 0.0067376283507661824}}\n",
      "Iteration 8: \n",
      "\t{'target': 0.994, 'params': {'beta_1': 0.8834609604734254, 'lr': 0.005631029301612942}}\n",
      "Iteration 9: \n",
      "\t{'target': 0.969, 'params': {'beta_1': 0.8280773877190468, 'lr': 0.0020612047419403}}\n",
      "Iteration 10: \n",
      "\t{'target': 0.987, 'params': {'beta_1': 0.9238248606315559, 'lr': 0.01}}\n",
      "Iteration 11: \n",
      "\t{'target': 0.1, 'params': {'beta_1': 1.0, 'lr': 0.0001}}\n",
      "Iteration 12: \n",
      "\t{'target': 0.886, 'params': {'beta_1': 0.9574847719448162, 'lr': 0.0001}}\n",
      "Iteration 13: \n",
      "\t{'target': 0.882, 'params': {'beta_1': 0.9060606688478187, 'lr': 0.0001}}\n",
      "Iteration 14: \n",
      "\t{'target': 0.889, 'params': {'beta_1': 0.8583786436774691, 'lr': 0.0001}}\n",
      "Iteration 15: \n",
      "\t{'target': 0.964, 'params': {'beta_1': 0.974588072127511, 'lr': 0.01}}\n",
      "Iteration 16: \n",
      "\t{'target': 0.956, 'params': {'beta_1': 0.9444577517990637, 'lr': 0.01}}\n",
      "Iteration 17: \n",
      "\t{'target': 0.899, 'params': {'beta_1': 0.901313725393386, 'lr': 0.01}}\n",
      "Iteration 18: \n",
      "\t{'target': 0.878, 'params': {'beta_1': 0.9334773444036866, 'lr': 0.0001}}\n",
      "Iteration 19: \n",
      "\t{'target': 0.88, 'params': {'beta_1': 0.9763409771195536, 'lr': 0.0001}}\n",
      "{'target': 0.998, 'params': {'beta_1': 0.800022874963469, 'lr': 0.003093092469055214}}\n"
     ]
    }
   ],
   "source": [
    "# Bounded region of parameter space\n",
    "pbounds = {'lr': (1e-4, 1e-2), 'beta_1': (0.8, 1)}\n",
    "\n",
    "# Create the bayesian optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=fit_with_partial,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "# Maximize the accuracy\n",
    "optimizer.maximize(init_points=10, n_iter=10,)\n",
    "\n",
    "# Print the result\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Perform Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "282/282 [==============================] - 22s 77ms/step - loss: 2.4378 - acc: 0.1541\n",
      "Epoch 2/5\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 2.1100 - acc: 0.2493\n",
      "Epoch 3/5\n",
      "282/282 [==============================] - 19s 68ms/step - loss: 1.8855 - acc: 0.3478\n",
      "Epoch 4/5\n",
      "282/282 [==============================] - 19s 69ms/step - loss: 1.6913 - acc: 0.4373\n",
      "Epoch 5/5\n",
      "282/282 [==============================] - 19s 69ms/step - loss: 1.5242 - acc: 0.5178\n",
      "Epoch 1/5\n",
      "282/282 [==============================] - 18s 64ms/step - loss: 1.6936 - acc: 0.4367\n",
      "Epoch 2/5\n",
      "282/282 [==============================] - 15s 54ms/step - loss: 0.8455 - acc: 0.7609\n",
      "Epoch 3/5\n",
      "282/282 [==============================] - 15s 54ms/step - loss: 0.5363 - acc: 0.8705\n",
      "Epoch 4/5\n",
      "282/282 [==============================] - 15s 54ms/step - loss: 0.3651 - acc: 0.9233\n",
      "Epoch 5/5\n",
      "282/282 [==============================] - 15s 55ms/step - loss: 0.2690 - acc: 0.9464\n",
      "Epoch 1/5\n",
      "282/282 [==============================] - 18s 65ms/step - loss: 0.6435 - acc: 0.8049\n",
      "Epoch 2/5\n",
      "282/282 [==============================] - 18s 63ms/step - loss: 0.0933 - acc: 0.9797\n",
      "Epoch 3/5\n",
      "282/282 [==============================] - 18s 64ms/step - loss: 0.0310 - acc: 0.9961\n",
      "Epoch 4/5\n",
      "282/282 [==============================] - 20s 70ms/step - loss: 0.0185 - acc: 0.9975\n",
      "Epoch 5/5\n",
      "282/282 [==============================] - 18s 65ms/step - loss: 0.0118 - acc: 0.9988\n",
      "Epoch 1/5\n",
      "282/282 [==============================] - 22s 78ms/step - loss: 2.7108 - acc: 0.1107\n",
      "Epoch 2/5\n",
      "282/282 [==============================] - 18s 65ms/step - loss: 2.2648 - acc: 0.1897\n",
      "Epoch 3/5\n",
      "282/282 [==============================] - 21s 74ms/step - loss: 2.0007 - acc: 0.2856\n",
      "Epoch 4/5\n",
      "282/282 [==============================] - 18s 65ms/step - loss: 1.8097 - acc: 0.3764\n",
      "Epoch 5/5\n",
      "282/282 [==============================] - 19s 69ms/step - loss: 1.6392 - acc: 0.4586\n",
      "Epoch 1/5\n",
      "282/282 [==============================] - 23s 81ms/step - loss: 1.6105 - acc: 0.4680\n",
      "Epoch 2/5\n",
      "282/282 [==============================] - 20s 70ms/step - loss: 0.8373 - acc: 0.7605\n",
      "Epoch 3/5\n",
      "282/282 [==============================] - 19s 67ms/step - loss: 0.5212 - acc: 0.8685\n",
      "Epoch 4/5\n",
      "282/282 [==============================] - 19s 67ms/step - loss: 0.3529 - acc: 0.9286\n",
      "Epoch 5/5\n",
      "282/282 [==============================] - 19s 68ms/step - loss: 0.2584 - acc: 0.9540\n",
      "Epoch 1/5\n",
      "282/282 [==============================] - 22s 78ms/step - loss: 0.6880 - acc: 0.7829\n",
      "Epoch 2/5\n",
      "282/282 [==============================] - 19s 67ms/step - loss: 0.0945 - acc: 0.9815\n",
      "Epoch 3/5\n",
      "282/282 [==============================] - 19s 68ms/step - loss: 0.0406 - acc: 0.9925\n",
      "Epoch 4/5\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.0134 - acc: 0.9994\n",
      "Epoch 5/5\n",
      "282/282 [==============================] - 15s 54ms/step - loss: 0.0075 - acc: 0.9998\n",
      "Epoch 1/5\n",
      "282/282 [==============================] - 18s 65ms/step - loss: nan - acc: 0.0997\n",
      "Epoch 2/5\n",
      "282/282 [==============================] - 15s 54ms/step - loss: nan - acc: 0.1007\n",
      "Epoch 3/5\n",
      "282/282 [==============================] - 15s 53ms/step - loss: nan - acc: 0.0997\n",
      "Epoch 4/5\n",
      "282/282 [==============================] - 15s 53ms/step - loss: nan - acc: 0.0993\n",
      "Epoch 5/5\n",
      "282/282 [==============================] - 15s 53ms/step - loss: nan - acc: 0.0998\n",
      "Epoch 1/5\n",
      "282/282 [==============================] - 19s 66ms/step - loss: nan - acc: 0.1012\n",
      "Epoch 2/5\n",
      "282/282 [==============================] - 15s 53ms/step - loss: nan - acc: 0.0991\n",
      "Epoch 3/5\n",
      "282/282 [==============================] - 15s 53ms/step - loss: nan - acc: 0.1012\n",
      "Epoch 4/5\n",
      "282/282 [==============================] - 15s 53ms/step - loss: nan - acc: 0.0996\n",
      "Epoch 5/5\n",
      "282/282 [==============================] - 18s 63ms/step - loss: nan - acc: 0.0995\n",
      "Epoch 1/5\n",
      "282/282 [==============================] - 19s 66ms/step - loss: nan - acc: 0.1004\n",
      "Epoch 2/5\n",
      "282/282 [==============================] - 15s 54ms/step - loss: nan - acc: 0.0993\n",
      "Epoch 3/5\n",
      "282/282 [==============================] - 15s 54ms/step - loss: nan - acc: 0.0995\n",
      "Epoch 4/5\n",
      "282/282 [==============================] - 15s 53ms/step - loss: nan - acc: 0.0998\n",
      "Epoch 5/5\n",
      "282/282 [==============================] - 15s 54ms/step - loss: nan - acc: 0.1021\n",
      "For learning rate = 1e-05 and momentum = 0.8: loss = 1.4151471729278564, accuracy = 0.568\n",
      "For learning rate = 0.0001 and momentum = 0.8: loss = 0.26280444824695587, accuracy = 0.937\n",
      "For learning rate = 0.001 and momentum = 0.8: loss = 0.008753713279962539, accuracy = 1.0\n",
      "For learning rate = 1e-05 and momentum = 0.9: loss = 1.5541619319915772, accuracy = 0.49\n",
      "For learning rate = 0.0001 and momentum = 0.9: loss = 0.2514191657304764, accuracy = 0.952\n",
      "For learning rate = 0.001 and momentum = 0.9: loss = 0.01832628623768687, accuracy = 0.996\n",
      "For learning rate = 1e-05 and momentum = 1.0: loss = nan, accuracy = 0.1\n",
      "For learning rate = 0.0001 and momentum = 1.0: loss = nan, accuracy = 0.1\n",
      "For learning rate = 0.001 and momentum = 1.0: loss = nan, accuracy = 0.1\n"
     ]
    }
   ],
   "source": [
    "lr_list = np.array([10**-5, 10**-4, 10**-3])\n",
    "mom_list = np.array([0.8, 0.9, 1.0])\n",
    "\n",
    "g_grid = np.meshgrid(lr_list, mom_list)\n",
    "g_grid_points = np.append(g_grid[0].reshape(-1,1), g_grid[1].reshape(-1,1), axis=1)\n",
    "\n",
    "g_result_list = []\n",
    "for i in g_grid_points:\n",
    "    model = create_model(lr=i[0], beta_1=i[1])\n",
    "    model.fit_generator(\n",
    "        generator=train_batch,\n",
    "        steps_per_epoch=len(train_batch),\n",
    "        epochs=5)\n",
    "    score = model.evaluate(X_test, Y_test, verbose = 0)\n",
    "    g_result_list.append([i[0], i[1], score[0], score[1]])\n",
    "\n",
    "for i in g_result_list:\n",
    "    print(\"For learning rate = {0} and momentum = {1}: loss = {2}, accuracy = {3}\".format(i[0], i[1], i[2], i[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Perform Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-9-e9805aee26e5>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-e9805aee26e5>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    print(\"For learning rate = {0} and momentum = {1}: loss = {2}, accuracy = {3}\".format(i[0], i[1], i[2], i[3]))\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "n_val = 9\n",
    "lr_list = 10 ** np.random.uniform(low=-5, high=-3, size=n_val)\n",
    "mom_list = np.random.uniform(low=0.8, high=1, size=n_val)\n",
    "\n",
    "r_grid_points = np.append(lr_list.reshape(-1,1), mom_list.reshape(-1,1), axis=1)\n",
    "\n",
    "r_result_list = []\n",
    "for i in r_grid_points:\n",
    "    model = create_model(lr=i[0], beta_1=i[1])\n",
    "    model.fit_generator(\n",
    "        generator=train_batch,\n",
    "        steps_per_epoch=len(train_batch),\n",
    "        epochs=5)\n",
    "    score = model.evaluate(X_test, Y_test, verbose = 0)\n",
    "    r_result_list.append([i[0], i[1], score[0], score[1]])\n",
    "\n",
    "for i in r_result_list:\n",
    "print(\"For learning rate = {0} and momentum = {1}: loss = {2}, accuracy = {3}\".format(i[0], i[1], i[2], i[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
